{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import qdrant\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "import qdrant_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the .env File from our directory to access API keys and endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to parse through PDF and return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf):\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(pdf)\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion to split full text into text-chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating/fetching Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(chunks, user_id, QDRANT_HOST, QDRANT_API_KEY):\n",
    "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "    vectorstore = qdrant.Qdrant.from_texts(texts=chunks, embedding=embeddings, url=QDRANT_HOST, api_key=QDRANT_API_KEY, collection_name=user_id)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching database if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_vector_store(user_id):\n",
    "    client = qdrant_client.QdrantClient(\n",
    "        os.getenv(\"QDRANT_HOST\"),\n",
    "        api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    "    )\n",
    "\n",
    "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")    \n",
    "\n",
    "    vectorstore = qdrant.Qdrant(\n",
    "        client=client,\n",
    "        collection_name=user_id,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        collection_info = client.get_collection(collection_name=user_id)\n",
    "        vectors_count = collection_info.vectors_count if collection_info else 0\n",
    "        # print(vectors_count)\n",
    "\n",
    "    except:\n",
    "        vectors_count=0\n",
    "\n",
    "    print(f\"Vectors count: {vectors_count}\")\n",
    "    \n",
    "    return vectorstore if vectors_count>0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation chain using Huggingface pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a conversation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_chain(vectorstore):\n",
    "    model_path = \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    generation_params = {\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 40,\n",
    "        \"max_new_tokens\": 1000,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    }\n",
    "    \n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, streamer=streamer, **generation_params)\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,        \n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final App Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing PDF and generate UI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_and_initialize_chat(pdf, QDRANT_HOST, QDRANT_API_KEY):\n",
    "    user_id = \"agoaoidg-2944agag\"  # Unique identifier for the user session\n",
    "    \n",
    "    # Process the PDF\n",
    "    raw_text = get_pdf_text(pdf)\n",
    "    text_chunks = get_text_chunks(raw_text)\n",
    "    \n",
    "    # Create vector store\n",
    "    vectorstore = fetch_vector_store(user_id)\n",
    "\n",
    "    if not vectorstore:\n",
    "        vectorstore = get_vector_store(text_chunks, user_id, QDRANT_HOST, QDRANT_API_KEY)\n",
    "    \n",
    "    # Initialize conversation chain\n",
    "    conversation_chain = get_conversation_chain(vectorstore)\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_pdf(user_question, conversation_chain):\n",
    "    prompt_template = f\"<s>[INST] {user_question} [/INST]\"\n",
    "    response = conversation_chain({'question': prompt_template})\n",
    "    \n",
    "    return response[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_app():\n",
    "    with gr.Blocks() as demo:\n",
    "        QDRANT_HOST = os.getenv(\"QDRANT_HOST\")\n",
    "        QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "        conversation_chain = None  # Store the conversation chain\n",
    "\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\"# Conversational AI \")\n",
    "\n",
    "        with gr.Row():\n",
    "            pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        \n",
    "        start_chat_button = gr.Button(\"Start Chat\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            user_question_input = gr.Textbox(label=\"Your Question\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            ai_response = gr.Textbox(label=\"AI Response\", interactive=False)\n",
    "\n",
    "        def start_chat(pdf):\n",
    "            nonlocal conversation_chain\n",
    "            conversation_chain = process_pdf_and_initialize_chat(pdf, QDRANT_HOST, QDRANT_API_KEY)\n",
    "            return \"Chat initialized. You can now ask questions!\"\n",
    "\n",
    "        def handle_question(user_question):\n",
    "            if conversation_chain is None:\n",
    "                return \"Please upload a PDF and start the chat first.\", \"\", \"\"\n",
    "            \n",
    "            response = chat_with_pdf(user_question, conversation_chain)\n",
    "            return response\n",
    "\n",
    "        start_chat_button.click(start_chat, inputs=[pdf_input], outputs=[ai_response])\n",
    "        submit_button.click(handle_question, inputs=[user_question_input], outputs=[ai_response])\n",
    "\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Vectors count: 0\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-09-19 13:50:05,049 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2024-09-19 13:50:19,648 - root - WARNING - Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "2024-09-19 13:50:37,171 - httpx - INFO - HTTP Request: POST https://f314c9c2-7b86-4820-9c7f-2ca2816f6e87.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/agoaoidg-2944agag/points/search \"HTTP/1.1 200 OK\"\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gross Floor Area (GFA) refers to the total square footage of the interior of a building, excluding areas like elevators, stairs, mechanical and electrical rooms, and loading docks. The calculation of GFA involves adding together the area of all habitable floors in the building, including mezzanine floors, basements, and attic spaces. It also includes the area of any lobbies, corridors, and staircases.\n",
      "\n",
      "The units used to measure GFA are typically square feet or square meters. In some cases, it may be necessary to convert from one unit to another based on the specific requirements of the project or location. The calculation of GFA should be done carefully, as it is a critical component of determining the size and cost of a building project.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/gradio/queueing.py\", line 521, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/gradio/blocks.py\", line 1513, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tejasram/miniconda3/envs/langchain_pdf/lib/python3.11/site-packages/gradio/utils.py\", line 832, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_14059/3677719098.py\", line 30, in handle_question\n",
      "    response, time_taken, tokens_per_sec = chat_with_pdf(user_question, conversation_chain)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: too many values to unpack (expected 3)\n"
     ]
    }
   ],
   "source": [
    "gradio_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
